{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNNGYtGI5DGb0Nt4FSDHKSt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rachit2005/UNET-/blob/main/video_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZsnymHB66Ef",
        "outputId": "72f765a1-224e-488f-a270-683abe678d7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'aggressive-behavior-video-classification' dataset.\n",
            "Path to dataset files: /kaggle/input/aggressive-behavior-video-classification\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"trainingdatapro/aggressive-behavior-video-classification\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms.v2 import RandomHorizontalFlip, RandomVerticalFlip, Compose, Normalize, Resize, ToPILImage\n",
        "from torch.utils.data import Dataset, ConcatDataset\n",
        "from torchvision.transforms import ToTensor\n",
        "from glob import glob\n",
        "from PIL import Image\n",
        "import shutil\n",
        "import torch\n",
        "import cv2\n",
        "import os\n",
        "import gc"
      ],
      "metadata": {
        "id": "7R5kQfoB8CNK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from cv2.gapi import video\n",
        "dataset_url = '/kaggle/input/aggressive-behavior-video-classification/files'\n",
        "\n",
        "def extract_frames(video_path, output_dir, frame_rate=1):\n",
        "  cap = cv2.VideoCapture(video_path)\n",
        "  frame_num = 0\n",
        "  frame_skip = 4\n",
        "\n",
        "  while True:\n",
        "    ret,frame = cap.read()\n",
        "    if not ret:\n",
        "      break\n",
        "    if frame_num % frame_skip == 0:\n",
        "      frame_path = os.path.join(output_dir , f'frame_{frame_num}.jpg')\n",
        "      cv2.imwrite(frame_path, frame)\n",
        "    frame_num += 1\n",
        "\n",
        "  cap.release() # Moved outside the loop\n",
        "\n",
        "\n",
        "def get_video(dataset_url):\n",
        "  video_paths = glob(os.path.join(dataset_url, '*.mp4'))\n",
        "  return video_paths\n"
      ],
      "metadata": {
        "id": "8kns_vnz8Y2V"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VideoDataset(Dataset):\n",
        "  def __init__(self,dataset_url , video_url , output_folder , transforms = None):\n",
        "    self.dataset_url = dataset_url\n",
        "    self.video_url = video_url\n",
        "    self.output_folder = output_folder\n",
        "    self.transforms = transforms\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    label = self.dataset_url.split('/')[-1]\n",
        "    video_path = self.video_url[index]\n",
        "\n",
        "    if self.transforms is not None:\n",
        "      os.makedirs(self.output_folder, exist_ok=True)\n",
        "      extract_frames(video_path, self.output_folder)\n",
        "\n",
        "      frames_list = [] # Initialize an empty list to collect frames\n",
        "\n",
        "      # Sort frame files to maintain temporal order if desired\n",
        "      frame_files = sorted([f for f in os.listdir(self.output_folder) if f.startswith('frame_') and f.endswith('.jpg')])\n",
        "\n",
        "      if not frame_files:\n",
        "          # Handle case where no frames were extracted (e.g., corrupt video or extract_frames failed)\n",
        "          # You might want to skip this item or return a placeholder/error\n",
        "          print(f\"Warning: No frames found for video: {video_path}\")\n",
        "          # For now, let's return a dummy tensor to allow the pipeline to continue, but a robust solution would handle this gracefully\n",
        "          dummy_video = torch.zeros(3, 1, 112, 112) # (C, D, H, W)\n",
        "          dummy_label = torch.tensor(0) # Or -1 for unknown\n",
        "          shutil.rmtree(self.output_folder) # Clean up\n",
        "          return dummy_video, dummy_label\n",
        "\n",
        "      for frame_file in frame_files:\n",
        "        frame_path = os.path.join(self.output_folder , frame_file)\n",
        "\n",
        "        image = Image.open(frame_path).convert(\"RGB\") # Ensure image is RGB\n",
        "\n",
        "        frame_tensor = self.transforms(image) # Apply transforms, should result in (C, H, W)\n",
        "        frames_list.append(frame_tensor)\n",
        "\n",
        "      # Stack all frames along a new dimension (depth/time) to get (D, C, H, W)\n",
        "      video_tensor = torch.stack(frames_list, dim=0)\n",
        "\n",
        "      # Permute to (C, D, H, W) as expected by Conv3d layers\n",
        "      video = video_tensor.permute(1, 0, 2, 3)\n",
        "\n",
        "      shutil.rmtree(self.output_folder) # Clean up temporary frames\n",
        "\n",
        "      if label == 'aggressive': # Corrected spelling for consistency, original was 'agressive'\n",
        "        label = torch.tensor(1)\n",
        "      else:\n",
        "        label = torch.tensor(0)\n",
        "\n",
        "      return video , label\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.video_url)\n"
      ],
      "metadata": {
        "id": "5kt3bx319OWJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a86440d8",
        "outputId": "16c02f30-b6c5-447c-9bff-4174676b48b2"
      },
      "source": [
        "output_agg = '/kaggle/working/agg_frames'\n",
        "output_non_agg = '/kaggle/working/non_agg_frames'\n",
        "\n",
        "effects = Compose([\n",
        "    ToPILImage(),\n",
        "    RandomHorizontalFlip(),\n",
        "    RandomVerticalFlip(),\n",
        "    Resize(size = (112, 112), antialias = True),\n",
        "    ToTensor(),\n",
        "    Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "output_agg = '/kaggle/working/agg_frames'\n",
        "output_non = '/kaggle/working/non_frames'\n",
        "\n",
        "agg_video_paths = get_video(os.path.join(dataset_url, 'aggressive'))\n",
        "nonagg_video_paths = get_video(os.path.join(dataset_url, 'non_aggressive'))\n",
        "\n",
        "agg_dataset = VideoDataset(os.path.join(dataset_url, 'aggressive'), agg_video_paths, output_agg,transforms=effects)\n",
        "nonagg_dataset = VideoDataset(os.path.join(dataset_url, 'non_aggressive'), nonagg_video_paths, output_non_agg,transforms=effects)\n",
        "\n",
        "dataset = ConcatDataset([agg_dataset, nonagg_dataset])\n",
        "\n",
        "print(\"Total videos in dataset:\", len(dataset))\n",
        "print(\"Dataset base path:\", dataset_url)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total videos in dataset: 11\n",
            "Dataset base path: /kaggle/input/aggressive-behavior-video-classification/files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Walk through the directory and print all files found\n",
        "for root, dirs, files in os.walk(path):\n",
        "    print(f\"Directory: {root}\")\n",
        "    for file in files:\n",
        "        print(f\"  - {file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NRcraavCU3V",
        "outputId": "7f172685-a5fb-479a-b703-121661085684"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory: /kaggle/input/aggressive-behavior-video-classification\n",
            "  - aggressive_behavior.csv\n",
            "Directory: /kaggle/input/aggressive-behavior-video-classification/files\n",
            "Directory: /kaggle/input/aggressive-behavior-video-classification/files/aggressive\n",
            "  - 3.mp4\n",
            "  - 1.mp4\n",
            "  - 4.mp4\n",
            "  - 0.mp4\n",
            "  - 2.mp4\n",
            "Directory: /kaggle/input/aggressive-behavior-video-classification/files/non_aggressive\n",
            "  - 5.mp4\n",
            "  - 3.mp4\n",
            "  - 1.mp4\n",
            "  - 4.mp4\n",
            "  - 0.mp4\n",
            "  - 2.mp4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "train_size = 8\n",
        "eval_size = 3\n",
        "\n",
        "train_dataset , test_dataset = random_split(dataset , [train_size, eval_size])\n",
        "\n",
        "batch_size = 1\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset , batch_size = batch_size , shuffle = True , num_workers=0)\n",
        "test_dataloder = DataLoader(test_dataset , batch_size , shuffle = True)\n",
        "\n",
        "# print(next(iter(train_dataloader)))\n",
        "\n",
        "print(len(train_dataloader.dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "sFFCx9_4AusY",
        "outputId": "e48089dc-f0eb-432c-f4a1-0f67b8a3673e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.ao.quantization as quantization\n",
        "\n",
        "class Conv3dBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Helper block: Conv3d -> BatchNorm3d -> ReLU\n",
        "    Structure designed for easy fusion during QAT.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(Conv3dBlock, self).__init__()\n",
        "        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
        "        self.bn = nn.BatchNorm3d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        return x\n",
        "\n",
        "class UNET_VIDEO_CLASSIFICATION(nn.Module):\n",
        "    def __init__(self, num_classes=10, in_channels=3):\n",
        "        super(UNET_VIDEO_CLASSIFICATION, self).__init__()\n",
        "\n",
        "        # --- Quantization Stubs ---\n",
        "        # QuantStub converts floating point tensors to quantized tensors\n",
        "        self.quant = quantization.QuantStub()\n",
        "        # DeQuantStub converts quantized tensors back to floating point\n",
        "        self.dequant = quantization.DeQuantStub()\n",
        "\n",
        "        # --- Contracting Path (Encoder) ---\n",
        "        # Level 1\n",
        "        self.enc1 = Conv3dBlock(in_channels, 32)\n",
        "        self.pool1 = nn.MaxPool3d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Level 2\n",
        "        self.enc2 = Conv3dBlock(32, 64)\n",
        "        self.pool2 = nn.MaxPool3d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Level 3\n",
        "        self.enc3 = Conv3dBlock(64, 128)\n",
        "        self.pool3 = nn.MaxPool3d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Level 4 (Bottleneck)\n",
        "        self.bottleneck = Conv3dBlock(128, 256)\n",
        "\n",
        "        # --- Classification Head ---\n",
        "        # Replaces the U-Net Decoder for classification tasks\n",
        "        self.global_pool = nn.AdaptiveAvgPool3d((1, 1, 1))\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1. Quantize inputs\n",
        "        x = self.quant(x)\n",
        "\n",
        "        # 2. Encoder Pass\n",
        "        x = self.pool1(self.enc1(x))\n",
        "\n",
        "        x = self.pool2(self.enc2(x))\n",
        "\n",
        "        x = self.pool3(self.enc3(x))\n",
        "\n",
        "        # 3. Bottleneck\n",
        "        x = self.bottleneck(x)\n",
        "\n",
        "        # 4. Classification Head\n",
        "        x = self.global_pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        # 5. Dequantize outputs\n",
        "        x = self.dequant(x)\n",
        "        return x\n",
        "\n",
        "    def fuse_model(self):\n",
        "        \"\"\"\n",
        "        Fuses Conv+BN+ReLU layers to save memory and improve speed.\n",
        "        Required for effective Quantization Aware Training.\n",
        "        \"\"\"\n",
        "        # Fuse the sub-blocks (Conv + BN + ReLU) by specifying full paths from the model\n",
        "        torch.ao.quantization.fuse_modules(\n",
        "            self, ['enc1.conv', 'enc1.bn', 'enc1.relu'], inplace=True\n",
        "        )\n",
        "        torch.ao.quantization.fuse_modules(\n",
        "            self, ['enc2.conv', 'enc2.bn', 'enc2.relu'], inplace=True\n",
        "        )\n",
        "        torch.ao.quantization.fuse_modules(\n",
        "            self, ['enc3.conv', 'enc3.bn', 'enc3.relu'], inplace=True\n",
        "        )\n",
        "        torch.ao.quantization.fuse_modules(\n",
        "            self, ['bottleneck.conv', 'bottleneck.bn', 'bottleneck.relu'], inplace=True\n",
        "        )"
      ],
      "metadata": {
        "id": "VLcbq9x-LfWz"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = UNET_VIDEO_CLASSIFICATION(num_classes=2)\n",
        "\n",
        "print(model)\n",
        "print(len(dataset))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "aA0H6D7OI5bO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75a44615-6e64-4b1d-83a1-b26cc93c75ce"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UNET_VIDEO_CLASSIFICATION(\n",
            "  (quant): QuantStub()\n",
            "  (dequant): DeQuantStub()\n",
            "  (enc1): Conv3dBlock(\n",
            "    (conv): Conv3d(3, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
            "    (bn): BatchNorm3d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "  )\n",
            "  (pool1): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (enc2): Conv3dBlock(\n",
            "    (conv): Conv3d(32, 64, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
            "    (bn): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "  )\n",
            "  (pool2): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (enc3): Conv3dBlock(\n",
            "    (conv): Conv3d(64, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
            "    (bn): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "  )\n",
            "  (pool3): MaxPool3d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (bottleneck): Conv3dBlock(\n",
            "    (conv): Conv3d(128, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), bias=False)\n",
            "    (bn): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (relu): ReLU(inplace=True)\n",
            "  )\n",
            "  (global_pool): AdaptiveAvgPool3d(output_size=(1, 1, 1))\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=2, bias=True)\n",
            ")\n",
            "11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantization aware training"
      ],
      "metadata": {
        "id": "uMlYl7oMJTaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import copy\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters() , lr = LR)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "def main():\n",
        "  BATCH_SIZE = 8\n",
        "  EPOCHS = 5\n",
        "  LR = 3e-4\n",
        "\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  model.eval() # Put model in eval mode for stable BatchNorm during fusion\n",
        "  model.fuse_model()\n",
        "  model.train() # Put model back in train mode for QAT preparation\n",
        "\n",
        "  # Move model to device BEFORE preparing for QAT\n",
        "  model.to(device)\n",
        "\n",
        "  # prepare for QAT\n",
        "  backend = \"fbgemm\"\n",
        "  model.qconfig = quantization.get_default_qat_qconfig(backend)\n",
        "  quantization.prepare_qat(model , inplace=True)\n",
        "\n",
        "\n",
        "  # training loop\n",
        "  print(\"starting the quantization aware training\")\n",
        "\n",
        "  for epoch in range(EPOCHS): # Corrected: Iterate over range(EPOCHS)\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0 # Initialize total for accuracy calculation\n",
        "\n",
        "    for videos, labels in train_dataloader:\n",
        "      videos = videos.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      output = model(videos)\n",
        "      loss = loss_fn(output , labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      total_loss += loss.item()\n",
        "      # Corrected: predicted should be the index of the max value\n",
        "      _, predicted = torch.max(output.data, 1)\n",
        "\n",
        "      total += labels.size(0)\n",
        "\n",
        "      correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_loss = total_loss / len(train_dataloader)\n",
        "    train_accuracy = 100 * correct / total\n",
        "    print(f\"epochs: {epoch+1}/{EPOCHS} | loss: {train_loss:.4f} | accuracy: {train_accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "jDnM_wDVDIVu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "id": "uxmX2HC48ZiR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "add8c5c8-9a05-40f7-d013-7ceaac5df6d8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting the quantization aware training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2897914237.py:20: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  quantization.prepare_qat(model , inplace=True)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epochs: 1/5 | loss: 0.6825 | accuracy: 50.00%\n",
            "epochs: 2/5 | loss: 0.6703 | accuracy: 62.50%\n",
            "epochs: 3/5 | loss: 0.6653 | accuracy: 62.50%\n",
            "epochs: 4/5 | loss: 0.6759 | accuracy: 62.50%\n",
            "epochs: 5/5 | loss: 0.6735 | accuracy: 62.50%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "model.to('cpu')\n",
        "\n",
        "quantized_model = quantization.convert(model , inplace=False) # replace QAT layers with INT8 layers\n",
        "print(\"conversion succesfull\")\n",
        "\n",
        "torch.save(quantized_model.state_dict(), 'quantized_model.pth')\n",
        "torch.save(model.state_dict() , \"model.pth\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bvKIAjswSmiD",
        "outputId": "d42e2388-61d4-421a-ee90-e7189eac19c7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "conversion succesfull\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-571915983.py:4: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
            "For migrations of users: \n",
            "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
            "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
            "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
            "see https://github.com/pytorch/ao/issues/2259 for more details\n",
            "  quantized_model = quantization.convert(model , inplace=False) # replace QAT layers with INT8 layers\n",
            "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/utils.py:435: UserWarning: must run observer before calling calculate_qparams. Returning default values.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total_loss = 0\n",
        "total = 0\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "with torch.no_grad():\n",
        "  for videos , labels in test_dataloder:\n",
        "    videos = videos.to('cpu')\n",
        "    labels = labels.to('cpu')\n",
        "\n",
        "    output = quantized_model(videos)\n",
        "    loss = loss_fn(output , labels)\n",
        "\n",
        "    _,pred = torch.max(output, 1)\n",
        "    correct += (pred == labels).sum().item()\n",
        "    total_loss += loss.item()\n",
        "    total += labels.size(0)\n",
        "\n",
        "  acc = 100 * correct / total\n",
        "\n",
        "  print(f\"correct: {correct} | accuracy: {acc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mLub_5XmXVJs",
        "outputId": "5694c8cf-0a56-40ff-eb1d-ec38c540d502"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "correct: 1 | accuracy: 33.333333333333336\n"
          ]
        }
      ]
    }
  ]
}